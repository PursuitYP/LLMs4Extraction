{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test OpenAI function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What's the result of 22 plus 5 in decimal added to the hexadecimal number A?\"\n",
      "    }\n",
      "]\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"id\": \"chatcmpl-7UW4JF5YKVVZDqjbGO12O9Ct8LUH6\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1687507467,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"add_decimal_values\",\n",
      "          \"arguments\": \"{\\n  \\\"value1\\\": 22,\\n  \\\"value2\\\": 5\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"function_call\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 154,\n",
      "    \"completion_tokens\": 25,\n",
      "    \"total_tokens\": 179\n",
      "  }\n",
      "}\n",
      "--------------------------------------------------\n",
      "22 + 5 = 27 (decimal)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What's the result of 22 plus 5 in decimal added to the hexadecimal number A?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "            \"name\": \"add_decimal_values\",\n",
      "            \"arguments\": \"{\\n  \\\"value1\\\": 22,\\n  \\\"value2\\\": 5\\n}\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"function\",\n",
      "        \"name\": \"add_decimal_values\",\n",
      "        \"content\": \"{\\\"result\\\": 27 }\"\n",
      "    }\n",
      "]\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"id\": \"chatcmpl-7UW4KoLkD0qUT624iv9pPSHw3oE3m\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1687507468,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "          \"name\": \"add_hexadecimal_values\",\n",
      "          \"arguments\": \"{\\n  \\\"value1\\\": \\\"27\\\",\\n  \\\"value2\\\": \\\"A\\\"\\n}\"\n",
      "        }\n",
      "      },\n",
      "      \"finish_reason\": \"function_call\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 194,\n",
      "    \"completion_tokens\": 26,\n",
      "    \"total_tokens\": 220\n",
      "  }\n",
      "}\n",
      "--------------------------------------------------\n",
      "27 + A = 31 (hex)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"What's the result of 22 plus 5 in decimal added to the hexadecimal number A?\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "            \"name\": \"add_decimal_values\",\n",
      "            \"arguments\": \"{\\n  \\\"value1\\\": 22,\\n  \\\"value2\\\": 5\\n}\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"function\",\n",
      "        \"name\": \"add_decimal_values\",\n",
      "        \"content\": \"{\\\"result\\\": 27 }\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": null,\n",
      "        \"function_call\": {\n",
      "            \"name\": \"add_hexadecimal_values\",\n",
      "            \"arguments\": \"{\\n  \\\"value1\\\": \\\"27\\\",\\n  \\\"value2\\\": \\\"A\\\"\\n}\"\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"function\",\n",
      "        \"name\": \"add_hexadecimal_values\",\n",
      "        \"content\": \"{\\\"result\\\": 31 }\"\n",
      "    }\n",
      "]\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"id\": \"chatcmpl-7UW4Mp6zcfI7E14dhX0eTX0pGALMN\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1687507470,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"The result of adding 22 and 5 in decimal, and then adding the hexadecimal number A, is 31.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 236,\n",
      "    \"completion_tokens\": 25,\n",
      "    \"total_tokens\": 261\n",
      "  }\n",
      "}\n",
      "--------------------------------------------------\n",
      "The result of adding 22 and 5 in decimal, and then adding the hexadecimal number A, is 31.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "COMPLETION_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "\n",
    "\n",
    "QUESTION = (\n",
    "    \"What's the result of 22 plus 5 in decimal added to the hexadecimal number A?\"\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": QUESTION},\n",
    "]\n",
    "\n",
    "\n",
    "def add_decimal_values(arguments):\n",
    "    value1 = int(re.search(r'\"value1\": (\\d+)', str(arguments)).group(1))\n",
    "    value2 = int(re.search(r'\"value2\": (\\d+)', str(arguments)).group(1))\n",
    "\n",
    "    result = value1 + value2\n",
    "    print(f\"{value1} + {value2} = {result} (decimal)\")\n",
    "\n",
    "    return value1 + value2\n",
    "\n",
    "\n",
    "def add_hexadecimal_values(arguments):\n",
    "    value1 = re.search(r'\"value1\": \"(\\w+)\"', str(arguments)).group(1)\n",
    "    value2 = re.search(r'\"value2\": \"(\\w+)\"', str(arguments)).group(1)\n",
    "\n",
    "    decimal1 = int(value1, 16)\n",
    "    decimal2 = int(value2, 16)\n",
    "\n",
    "    result = hex(decimal1 + decimal2)[2:]\n",
    "    print(f\"{value1} + {value2} = {result} (hex)\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_completion(messages):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=COMPLETION_MODEL,\n",
    "        messages=messages,\n",
    "        functions=[\n",
    "            {\n",
    "                \"name\": \"add_decimal_values\",\n",
    "                \"description\": \"Add two decimal values\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"value1\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"The first decimal value to add. For example, 5\",\n",
    "                        },\n",
    "                        \"value2\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"The second decimal value to add. For example, 10\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"value1\", \"value2\"],\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"add_hexadecimal_values\",\n",
    "                \"description\": \"Add two hexadecimal values\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"value1\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The first hexadecimal value to add. For example, 5\",\n",
    "                        },\n",
    "                        \"value2\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The second hexadecimal value to add. For example, A\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"value1\", \"value2\"],\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "while True:\n",
    "    print(json.dumps(messages, indent=4))\n",
    "    print(\"-\" * 50)\n",
    "    response = get_completion(messages)\n",
    "    print(response)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if response.choices[0][\"finish_reason\"] == \"stop\":\n",
    "        print(response.choices[0][\"message\"][\"content\"])\n",
    "        break\n",
    "\n",
    "    elif response.choices[0][\"finish_reason\"] == \"function_call\":\n",
    "        fn_name = response.choices[0].message[\"function_call\"].name\n",
    "        arguments = response.choices[0].message[\"function_call\"].arguments\n",
    "\n",
    "        function = locals()[fn_name]\n",
    "        result = function(arguments)\n",
    "        print(\"-\" * 100)\n",
    "\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": None,\n",
    "                \"function_call\": {\n",
    "                    \"name\": fn_name,\n",
    "                    \"arguments\": arguments,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\", \n",
    "                \"name\": fn_name, \n",
    "                \"content\": f'{{\"result\": {str(result)} }}'}\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Pydantic + Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from openai_function_call import openai_function\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "@openai_function\n",
    "def sum(a:int, b:int) -> int:\n",
    "    \"\"\"Sum description adds a + b\"\"\"\n",
    "    return a + b\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        temperature=0,\n",
    "        functions=[sum.openai_schema],\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You must use the `sum` function instead of adding yourself.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is 6+3 use the `sum` function\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "result = sum.from_response(completion)\n",
    "print(result)  # 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' age=30\n",
      "{'name': 'John Doe', 'age': 30}\n",
      "{\n",
      "    \"name\": \"John Doe\",\n",
      "    \"age\": 30\n",
      "}\n",
      "--------------------------------------------------\n",
      "{\n",
      "    \"title\": \"UserDetails\",\n",
      "    \"description\": \"User Details\",\n",
      "    \"type\": \"object\",\n",
      "    \"properties\": {\n",
      "        \"name\": {\n",
      "            \"description\": \"User's name\",\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        \"age\": {\n",
      "            \"description\": \"User's age\",\n",
      "            \"type\": \"integer\"\n",
      "        }\n",
      "    },\n",
      "    \"required\": [\n",
      "        \"name\",\n",
      "        \"age\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from pydantic import Field, BaseModel\n",
    "from openai_function_call import OpenAISchema\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"http_proxy\"] = \"http://10.10.10.10:17890\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.10.10.10:17890\"\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "class UserDetails(OpenAISchema):\n",
    "    \"\"\"User Details\"\"\"\n",
    "    name: str = Field(..., description=\"User's name\")\n",
    "    age: int = Field(..., description=\"User's age\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    functions=[UserDetails.openai_schema],\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"I'm going to ask for user details. Use UserDetails to parse this data.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"My name is John Doe and I'm 30 years old.\"\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "user_details = UserDetails.from_response(completion)\n",
    "print(user_details)  # UserDetails(name=\"John Doe\", age=30)\n",
    "print(dict(user_details))\n",
    "print(json.dumps(dict(user_details), indent=4))\n",
    "print(\"-\" * 50)\n",
    "print(UserDetails.schema_json(indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code_String Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "section_name: str = Field(..., description=\"section name\")\n",
      "location_of_the_samples_and_sections: str = Field(..., description=\"location of the samples and sections\")\n",
      "GPS_location: str = Field(..., description=\"GPS location\")\n",
      "associated_fossils: str = Field(..., description=\"associated fossils\")\n",
      "lithology: str = Field(..., description=\"lithology\")\n",
      "number_of_species_and_genera_found: str = Field(..., description=\"number of species and genera found\")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "name: str = Field(..., description=\"User's name\")\n",
      "age: int = Field(..., description=\"User's age\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "field_list = [\"section name\", \"location of the samples and sections\", \"GPS location\", \n",
    "              \"associated fossils\", \"lithology\", \"number of species and genera found\"]\n",
    "attribute_set_string = \"\"\n",
    "for idx, field in enumerate(field_list):\n",
    "        new_attribute = f\"\"\"{field.replace(\" \", \"_\")}: str = Field(..., description=\"{field}\")\"\"\"\n",
    "        attribute_set_string = attribute_set_string + \"\\n\" + new_attribute\n",
    "print(attribute_set_string)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# name: str = Field(..., description=\"User's name\")\n",
    "# age: int = Field(..., description=\"User's age\")\n",
    "\n",
    "attr1 = \"name\"\n",
    "attr2 = \"age\"\n",
    "code_string = f\"\"\"{attr1}: str = Field(..., description=\"User's name\")\n",
    "{attr2}: int = Field(..., description=\"User's age\")\n",
    "\"\"\"\n",
    "print(code_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Extraction w/o Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"section name\": [\n",
      "        \"Abstract\",\n",
      "        \"Kunga Island section\"\n",
      "    ],\n",
      "    \"location of the samples and sections\": [\n",
      "        \"Kunga Island, Queen Charlotte Islands (QCI), and Inuyama\",\n",
      "        \"Queen Charlotte Islands, B.C. (Canada) and Inuyama\",\n",
      "        \"Queen Charlotte Islands, Canada\",\n",
      "        \"Queen Charlotte Islands, Canada; Inuyama, Japan\",\n",
      "        \"Queen Charlotte Islands, Canada; Inuyama, Kuzuu and Ikuno, Japan; New Zealand; Montenegro\"\n",
      "    ],\n",
      "    \"GPS location\": [],\n",
      "    \"associated fossils\": [],\n",
      "    \"lithology\": [],\n",
      "    \"number of species and genera found\": [\n",
      "        \"Nearly 20 genera and over 130 Rhaetian species disappeared at the end of the Triassic\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LLMs for DeepShovel: 结构化数据抽取 \"\"\"\n",
    "import os\n",
    "import openai\n",
    "import textract\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PyPDF2 import PdfReader\n",
    "import ast\n",
    "import json\n",
    "\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \"\"\"Yield successive n-sized chunks from text.\"\"\"\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "# 使用gpt-3.5-turbo抽取数据，加入异常处理机制\n",
    "def extract_chunk(document, template_prompt):\n",
    "    for i in range(3):  # Retry the API call up to 3 times\n",
    "        try:\n",
    "            prompt=template_prompt.replace('<document>', document)\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model='gpt-3.5-turbo', \n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=1500,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0\n",
    "            )\n",
    "            return \"1. \" + response['choices'][0]['message']['content']\n",
    "        except openai.error.RateLimitError:  # If rate limit is exceeded\n",
    "            wait_time = (2 ** i) + random.random()  # Exponential backoff with jitter\n",
    "            logging.warning(f\"Rate limit exceeded. Retrying after {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        except Exception as e:  # If any other error occurs\n",
    "            logging.error(f\"API call failed: {str(e)}\")\n",
    "            return None  # Return None for failure\n",
    "    logging.error(\"Failed to call OpenAI API after multiple retries due to rate limiting.\")\n",
    "    return None  # Return None for failure\n",
    "\n",
    "\n",
    "# 调用API并使用重试机制处理rate limit error和其他异常\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    for i in range(3):  # Retry the API call up to 3 times\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "            )\n",
    "            return response.choices[0].message[\"content\"]\n",
    "        except openai.error.RateLimitError:  # If rate limit is exceeded\n",
    "            wait_time = (2 ** i) + random.random()  # Exponential backoff with jitter\n",
    "            logging.warning(f\"Rate limit exceeded. Retrying after {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        except Exception as e:  # If any other error occurs\n",
    "            logging.error(f\"API call failed: {str(e)}\")\n",
    "            return None  # Return None for failure\n",
    "    logging.error(\"Failed to call OpenAI API after multiple retries due to rate limiting.\")\n",
    "    return None  # Return None for failure\n",
    "\n",
    "\n",
    "# 处理过程信息写入log文件\n",
    "def log_to_file(log_file, message):\n",
    "    try:\n",
    "        with open(log_file, 'a') as file:\n",
    "            file.write(message + '\\n')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed to log to file {log_file}: {str(e)}')\n",
    "        raise\n",
    "\n",
    "\n",
    "# 使用PdfReader读取pdf文献，手动加入Page Number信息\n",
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "# 传入pdf路径和带抽取的属性列表，返回抽取的结构化数据\n",
    "def data_extraction(pdf_path, field_list):\n",
    "    # 1. 执行pdf解析和切片\n",
    "    pdf_text = read_pdf(pdf_path)\n",
    "    clean_text = pdf_text.replace(\"  \", \" \").replace(\"\\n\", \"; \").replace(';',' ')\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    chunks = create_chunks(clean_text, 1000, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "    # 2. 关键信息抽取：多线程对text_chunks处理，抽取关键信息\n",
    "    question_format = \"0. What is the value of the 'title' attribute\"\n",
    "    # 适应性地生成抽取问题，并集成到抽取提示extract_prompt中去\n",
    "    for idx, field in enumerate(field_list):\n",
    "        new_question = str(idx+1) + \". What is the value of the '\" + field + \"' attribute\"\n",
    "        question_format = question_format + \"\\n\" + new_question\n",
    "    document = '<document>'\n",
    "    # 关键信息抽取prompt\n",
    "    extract_prompt=f'''Extract key pieces of information from this regulation document.\n",
    "If a particular piece of information is not present, output \\\"Not specified\\\".\n",
    "When you extract a key piece of information, include the closest page number.\n",
    "---\n",
    "Use the following format:\n",
    "{question_format}\n",
    "---\n",
    "Document: \\\"\\\"\\\"{document}\\\"\\\"\\\"\\n\n",
    "0. What is the value of the 'title' attribute: Origin of Lower Carboniferous cherts in southern Guizhou, South China (Page 1)\n",
    "1.'''\n",
    "    # 多线程对text_chunks处理，抽取关键信息\n",
    "    results = []\n",
    "    log_file = 'log_geo_extract.txt'\n",
    "    # log_to_file(log_file, f'Number of chunks: {len(text_chunks)}')\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # 多线程处理\n",
    "        futures = {executor.submit(extract_chunk, chunk, extract_prompt): chunk for chunk in text_chunks}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc='Processing chunks'):\n",
    "            # 收集完成的线程处理好的结果\n",
    "            response = future.result()\n",
    "            if response is None:\n",
    "                # log_to_file(log_file, f'Failed to process chunk {futures[future]}')\n",
    "                pass\n",
    "            else:\n",
    "                # 汇总关键信息抽取的结果\n",
    "                results.append(response)\n",
    "                # log_to_file(log_file, f'Successfully processed chunk!')\n",
    "    # 进一步整理关键信息抽取结果，便于下一步格式化转换\n",
    "    groups = [r.split('\\n') for r in results]\n",
    "    groups = [y for x in groups for y in x]\n",
    "    groups = sorted(groups)\n",
    "    groups = [x for x in groups if \"Not specified\" not in x and \"__\" not in x]\n",
    "    zipped = groups\n",
    "    # 移除太长的结果 (保留len(r) <= 180)\n",
    "    zipped = [r for r in zipped if len(r) <= 180]\n",
    "\n",
    "    # 3. 数据格式转换：根据抽取的关键信息，转换生成JSON样式的结果\n",
    "    zipped_example = [\"1. What is the value of the 'section name' attribute: The end-Triassic extinction event (ETE) (Page 1)\", \"1. What is the value of the 'section name' attribute: Katsuyama section (Page 2)\", \"1. What is the value of the 'section name' attribute: The Inuyama area (Page 1)\", \"2. What is the value of the 'location of the samples and sections' attribute: Katsuyama section, Inuyama, Japan (Page 1)\", \"2. What is the value of the 'location of the samples and sections' attribute: Inuyama area, central Japan (Page 2)\", \"2. What is the value of the 'location of the samples and sections' attribute: Rock samples from TJ-3 to TJ + 4 (3 beds above TJ + 1) continuously (Page 2)\", \"3. What is the value of the 'GPS location' attribute: N 35◦25.367′, E 136◦58.261 (Page 2)\", \"4. What is the value of the 'associated fossils' attribute: Sea surface-dwelling radiolaria (Page 1)\", \"4. What is the value of the 'associated fossils' attribute: Radiolarian fossils (Page 1)\", \"4. What is the value of the 'associated fossils' attribute: Radiolarian fossils (Page 3)\", \"5. What is the value of the 'lithology' attribute: Bedded chert (Page 1)\", \"5. What is the value of the 'lithology' attribute: Bedded chert and siliciclastic rocks (Page 2)\", \"5. What is the value of the 'lithology' attribute: Siliceous mudstone, bedded chert sequence, and siliciclastic rocks (Page 1)\"]\n",
    "    zipped_str_example = str(zipped_example)[1:][:-1]\n",
    "    field_list_example = [\"section name\", \"location of the samples and sections\", \"GPS location\", \n",
    "                          \"associated fossils\", \"lithology\", \"number of species and genera found\"]\n",
    "    zipped_str = str(zipped)[1:][:-1]\n",
    "    # 数据格式转换prompt\n",
    "    transform_prompt = f'''You will read a paragraph, summarise it in JSON format according to keywords and remove duplicate values.\n",
    "---\n",
    "Here is an example: \n",
    "\n",
    "PARAGRAPH\n",
    "{zipped_str_example}\n",
    "KEYWORDS\n",
    "{field_list_example}\n",
    "OUTPUT\n",
    "{{\n",
    "    \"section name\": [\n",
    "        \"The end-Triassic extinction event (ETE)\",\n",
    "        \"Katsuyama section\",\n",
    "        \"The Inuyama area\"\n",
    "    ],\n",
    "    \"location of the samples and sections\": [\n",
    "        \"Katsuyama section, Inuyama, Japan\",\n",
    "        \"Inuyama area, central Japan\",\n",
    "        \"Rock samples from TJ-3 to TJ + 4 (3 beds above TJ + 1) continuously\"\n",
    "    ],\n",
    "    \"GPS location\": [\n",
    "        \"N 35◦25.367′, E 136◦58.261\"\n",
    "    ],\n",
    "    \"associated fossils\": [\n",
    "        \"Sea surface-dwelling radiolaria\",\n",
    "        \"Radiolarian fossils\"\n",
    "    ],\n",
    "    \"lithology\": [\n",
    "        \"Bedded chert\",\n",
    "        \"Bedded chert and siliciclastic rocks\",\n",
    "        \"Siliceous mudstone, bedded chert sequence, and siliciclastic rocks\"\n",
    "    ],\n",
    "    \"number of species and genera found\": []\n",
    "}}\n",
    "---\n",
    "Here is the paragragh you need to process, summarise it in JSON format according to keywords and remove duplicate values: \n",
    "\n",
    "PARAGRAPH\n",
    "{zipped_str}\n",
    "KEYWORDS\n",
    "{field_list}\n",
    "OUTPUT\n",
    "\n",
    "'''\n",
    "\n",
    "    response = get_completion(transform_prompt)\n",
    "    res_json = ast.literal_eval(response)\n",
    "\n",
    "    return res_json\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 环境初始化，用户上传OpenAI API key\n",
    "    load_dotenv()\n",
    "    os.environ[\"http_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "    os.environ[\"https_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "    # Load your API key from an environment variable or secret management service\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    os.environ['OPENAI_API_KEY'] = openai.api_key\n",
    "\n",
    "    # 用户上传pdf，输入需要抽取的属性列表\n",
    "    pdf_path = \"data/radiolarian/000.pdf\"\n",
    "    field_list = [\"section name\", \"location of the samples and sections\", \"GPS location\", \n",
    "                  \"associated fossils\", \"lithology\", \"number of species and genera found\"]\n",
    "    \n",
    "    # LLMs结构化数据抽取\n",
    "    res_json = data_extraction(pdf_path, field_list)\n",
    "    with open('results/result.json', 'w', newline='\\n') as file:\n",
    "        json.dump(res_json, file, indent=4)\n",
    "\n",
    "    print(json.dumps(res_json, indent=4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Extraction w/ Function Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 12/12 [00:16<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"section_name\": \"5.3 Buryella tetradica -Bekoma campechensis interval zone (Page 7)\",\n",
      "    \"location_of_the_samples_and_sections\": \"Greater Indian passive continental margin (Page 21966)\",\n",
      "    \"GPS_location\": \"\",\n",
      "    \"associated_fossils\": \"Bekoma campechensis, Buryella tetradica, B. pentadica, Clathrocycloma (?) parcum, C. aff. catherinea (Page 7)\",\n",
      "    \"lithology\": \"burgundy and gray laminated siliceous shale and siliceous rocks (Page 5)\",\n",
      "    \"number_of_species_and_genera_found\": \"54 species of 30 radiolarian genera (Page 5)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LLMs for DeepShovel: 结构化数据抽取 \"\"\"\n",
    "import os\n",
    "import openai\n",
    "import textract\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PyPDF2 import PdfReader\n",
    "import ast\n",
    "import json\n",
    "from pydantic import Field, BaseModel\n",
    "from openai_function_call import OpenAISchema\n",
    "\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \"\"\"Yield successive n-sized chunks from text.\"\"\"\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "# 使用gpt-3.5-turbo抽取数据，加入异常处理机制\n",
    "def extract_chunk(document, template_prompt):\n",
    "    for i in range(3):  # Retry the API call up to 3 times\n",
    "        try:\n",
    "            prompt=template_prompt.replace('<document>', document)\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model='gpt-3.5-turbo', \n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                max_tokens=1500,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0\n",
    "            )\n",
    "            return \"1. \" + response['choices'][0]['message']['content']\n",
    "        except openai.error.RateLimitError:  # If rate limit is exceeded\n",
    "            wait_time = (2 ** i) + random.random()  # Exponential backoff with jitter\n",
    "            logging.warning(f\"Rate limit exceeded. Retrying after {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        except Exception as e:  # If any other error occurs\n",
    "            logging.error(f\"API call failed: {str(e)}\")\n",
    "            return None  # Return None for failure\n",
    "    logging.error(\"Failed to call OpenAI API after multiple retries due to rate limiting.\")\n",
    "    return None  # Return None for failure\n",
    "\n",
    "\n",
    "# 调用API并使用重试机制处理rate limit error和其他异常\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    for i in range(3):  # Retry the API call up to 3 times\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "            )\n",
    "            return response.choices[0].message[\"content\"]\n",
    "        except openai.error.RateLimitError:  # If rate limit is exceeded\n",
    "            wait_time = (2 ** i) + random.random()  # Exponential backoff with jitter\n",
    "            logging.warning(f\"Rate limit exceeded. Retrying after {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        except Exception as e:  # If any other error occurs\n",
    "            logging.error(f\"API call failed: {str(e)}\")\n",
    "            return None  # Return None for failure\n",
    "    logging.error(\"Failed to call OpenAI API after multiple retries due to rate limiting.\")\n",
    "    return None  # Return None for failure\n",
    "\n",
    "# 调用API并使用重试机制处理rate limit error和其他异常，调用function call功能\n",
    "def get_completion_function_call(prompt, attribute_set_string):\n",
    "    class AttributeDict(OpenAISchema):\n",
    "        \"\"\"Attributes of user input\"\"\"\n",
    "        exec(attribute_set_string)\n",
    "\n",
    "    for i in range(3):  # Retry the API call up to 3 times\n",
    "        try:\n",
    "            messages = [{\"role\": \"system\", \"content\": \"Use AttributeDict to parse this data.\"}, \n",
    "                        {\"role\": \"user\", \"content\": prompt}]\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo-0613\",\n",
    "                functions = [AttributeDict.openai_schema],\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "            )\n",
    "            return response\n",
    "        except openai.error.RateLimitError:  # If rate limit is exceeded\n",
    "            wait_time = (2 ** i) + random.random()  # Exponential backoff with jitter\n",
    "            logging.warning(f\"Rate limit exceeded. Retrying after {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        except Exception as e:  # If any other error occurs\n",
    "            logging.error(f\"API call failed: {str(e)}\")\n",
    "            return None  # Return None for failure\n",
    "    logging.error(\"Failed to call OpenAI API after multiple retries due to rate limiting.\")\n",
    "    return None  # Return None for failure\n",
    "\n",
    "\n",
    "# 处理过程信息写入log文件\n",
    "def log_to_file(log_file, message):\n",
    "    try:\n",
    "        with open(log_file, 'a') as file:\n",
    "            file.write(message + '\\n')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed to log to file {log_file}: {str(e)}')\n",
    "        raise\n",
    "\n",
    "\n",
    "# 使用PdfReader读取pdf文献，手动加入Page Number信息\n",
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "# 传入pdf路径和带抽取的属性列表，返回抽取的结构化数据\n",
    "def data_extraction(pdf_path, field_list):\n",
    "    # 1. 执行pdf解析和切片\n",
    "    pdf_text = read_pdf(pdf_path)\n",
    "    clean_text = pdf_text.replace(\"  \", \" \").replace(\"\\n\", \"; \").replace(';',' ')\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    chunks = create_chunks(clean_text, 1000, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "    # 2. 关键信息抽取：多线程对text_chunks处理，抽取关键信息\n",
    "    question_format = \"0. What is the value of the 'title' attribute\"\n",
    "    # 适应性地生成抽取问题，并集成到抽取提示extract_prompt中去\n",
    "    for idx, field in enumerate(field_list):\n",
    "        new_question = str(idx+1) + \". What is the value of the '\" + field + \"' attribute\"\n",
    "        question_format = question_format + \"\\n\" + new_question\n",
    "    document = '<document>'\n",
    "    # 关键信息抽取prompt\n",
    "    extract_prompt=f'''Extract key pieces of information from this regulation document.\n",
    "If a particular piece of information is not present, output \\\"Not specified\\\".\n",
    "When you extract a key piece of information, include the closest page number.\n",
    "---\n",
    "Use the following format:\n",
    "{question_format}\n",
    "---\n",
    "Document: \\\"\\\"\\\"{document}\\\"\\\"\\\"\\n\n",
    "0. What is the value of the 'title' attribute: Origin of Lower Carboniferous cherts in southern Guizhou, South China (Page 1)\n",
    "1.'''\n",
    "    # 多线程对text_chunks处理，抽取关键信息\n",
    "    results = []\n",
    "    log_file = 'log_geo_extract.txt'\n",
    "    # log_to_file(log_file, f'Number of chunks: {len(text_chunks)}')\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # 多线程处理\n",
    "        futures = {executor.submit(extract_chunk, chunk, extract_prompt): chunk for chunk in text_chunks}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc='Processing chunks'):\n",
    "            # 收集完成的线程处理好的结果\n",
    "            response = future.result()\n",
    "            if response is None:\n",
    "                # log_to_file(log_file, f'Failed to process chunk {futures[future]}')\n",
    "                pass\n",
    "            else:\n",
    "                # 汇总关键信息抽取的结果\n",
    "                results.append(response)\n",
    "                # log_to_file(log_file, f'Successfully processed chunk!')\n",
    "    # 进一步整理关键信息抽取结果，便于下一步格式化转换\n",
    "    groups = [r.split('\\n') for r in results]\n",
    "    groups = [y for x in groups for y in x]\n",
    "    groups = sorted(groups)\n",
    "    groups = [x for x in groups if \"Not specified\" not in x and \"__\" not in x]\n",
    "    zipped = groups\n",
    "    # 移除太长的结果 (保留len(r) <= 180)\n",
    "    zipped = [r for r in zipped if len(r) <= 180]\n",
    "\n",
    "    # 3. 数据格式转换：根据抽取的关键信息，转换生成JSON样式的结果\n",
    "    zipped_str = str(zipped)[1:][:-1]\n",
    "    # 数据格式转换prompt\n",
    "    transform_prompt = f'''I'm going to ask for attributes. Use AttributeDict to parse this data.\"\n",
    "---\n",
    "PARAGRAPH\n",
    "{zipped_str}\n",
    "'''\n",
    "\n",
    "    # 适应性地生成attribute_set_string, 在AttributeDict类中exec()生成可执行代码\n",
    "    attribute_set_string = \"\"\n",
    "    for idx, field in enumerate(field_list):\n",
    "        new_attribute = f\"\"\"{field.replace(\" \", \"_\")}: str = Field(..., description=\"{field}\")\"\"\"\n",
    "        attribute_set_string = attribute_set_string + \"\\n\" + new_attribute\n",
    "\n",
    "    class AttributeDict(OpenAISchema):\n",
    "        \"\"\"Attributes of user input\"\"\"\n",
    "        exec(attribute_set_string)\n",
    "\n",
    "    response = get_completion_function_call(transform_prompt, attribute_set_string)\n",
    "    arrtibutes = AttributeDict.from_response(response)\n",
    "    res_json = dict(arrtibutes)\n",
    "\n",
    "    return res_json\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 环境初始化，用户上传OpenAI API key\n",
    "    load_dotenv()\n",
    "    os.environ[\"http_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "    os.environ[\"https_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "    # os.environ[\"http_proxy\"] = \"http://10.10.10.10:17890\"\n",
    "    # os.environ[\"https_proxy\"] = \"http://10.10.10.10:17890\"\n",
    "    # Load your API key from an environment variable or secret management service\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    os.environ['OPENAI_API_KEY'] = openai.api_key\n",
    "\n",
    "    # 用户上传pdf，输入需要抽取的属性列表\n",
    "    pdf_path = \"data/radiolarian/466.pdf\"\n",
    "    field_list = [\"section name\", \"location of the samples and sections\", \"GPS location\", \n",
    "                  \"associated fossils\", \"lithology\", \"number of species and genera found\"]\n",
    "\n",
    "    # LLMs结构化数据抽取\n",
    "    res_json = data_extraction(pdf_path, field_list)\n",
    "    with open('results/result.json', 'w', newline='\\n') as file:\n",
    "        json.dump(res_json, file, indent=4)\n",
    "\n",
    "    print(json.dumps(res_json, indent=4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test sentence similarity prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "# 调用API并使用重试机制处理rate limit error和其他异常\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    for i in range(3):  # Retry the API call up to 3 times\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "            )\n",
    "            return response.choices[0].message[\"content\"]\n",
    "        except openai.error.RateLimitError:  # If rate limit is exceeded\n",
    "            wait_time = (2 ** i) + random.random()  # Exponential backoff with jitter\n",
    "            logging.warning(f\"Rate limit exceeded. Retrying after {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        except Exception as e:  # If any other error occurs\n",
    "            logging.error(f\"API call failed: {str(e)}\")\n",
    "            return None  # Return None for failure\n",
    "    logging.error(\"Failed to call OpenAI API after multiple retries due to rate limiting.\")\n",
    "    return None  # Return None for failure\n",
    "\n",
    "\n",
    "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "sentence2 = \"The quick yellow fox jumps over the lazy cat.\"\n",
    "prompt = f'''Please decide whether the following two sentences are semantically related.\n",
    "Your output can only be relevant or irrelevant.\n",
    "---\n",
    "SENTENCE 1: {sentence1}\n",
    "SENTENCE 2: {sentence2}\n",
    "OUTPUT\n",
    "'''\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test revChatGPT.V3 chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from revChatGPT.V3 import Chatbot\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"http_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = openai.api_key\n",
    "\n",
    "# basic example\n",
    "# chatbot = Chatbot(api_key=openai.api_key)\n",
    "# chatbot.ask(\"Hello world\")\n",
    "\n",
    "# streaming example\n",
    "chatbot = Chatbot(api_key=openai.api_key)\n",
    "for data in chatbot.ask_stream(\"Hello world\"):\n",
    "    print(data, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test single paper extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total paper count: 0,  total sentence count: 345\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "pdf_text = read_pdf(\"data/Favorable Effects of Tacrolimus Monotherapy on Myasthenia Gravis Patients.pdf\")\n",
    "str_tmp = pdf_text.replace(\"  \", \" \").replace(\"\\n\", \"; \").replace(';',' ')\n",
    "\n",
    "sentences_list = []\n",
    "paper_idx = 0\n",
    "paper_cnt = 0\n",
    "sentence_cnt = 0\n",
    "sentence_ls = str_tmp.split('. ')\n",
    "for sentence in sentence_ls:\n",
    "    # 正则过滤字符串，只保留空格、数字、字母、百分数、-、<、>、/、小数点等\n",
    "    sentence = re.sub(r'[^ \\dA-Za-z%<>/\\.-]', '', sentence)\n",
    "    # 如果句子太长，切分处理\n",
    "    if len(sentence) > 500:\n",
    "        idx = 0\n",
    "        while idx < len(sentence):\n",
    "            tmp_str = sentence[idx: min(idx + 500, len(sentence))] + \" end\"\n",
    "            if len(tmp_str) <= 8:\n",
    "                tmp_str += ' end'\n",
    "            if tmp_str[-1] != \".\":\n",
    "                tmp_str += '.'\n",
    "            sentences_list.append(tmp_str)\n",
    "            sentence_cnt += 1\n",
    "            idx += 500\n",
    "    else:\n",
    "        if len(sentence) <= 8:\n",
    "            sentence += ' end'\n",
    "        if sentence[-1] != \".\":\n",
    "            sentence += '.'\n",
    "        sentences_list.append(sentence)\n",
    "        sentence_cnt += 1\n",
    "print(\"# total paper count: {},  total sentence count: {}\".format(paper_cnt, sentence_cnt))\n",
    "\n",
    "write_file = \"../../engineering/mgkg/sentences/sentence_display.txt\"\n",
    "with open(write_file, 'w') as f:\n",
    "    for item in sentences_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "# extract sentences in title, abstract, and sections from retrieved data indexed between [left_index, right_index)\n",
    "def extract_sentences():\n",
    "    sentences_list = []\n",
    "    paper_idx = 0\n",
    "    paper_cnt = 0\n",
    "    sentence_cnt = 0\n",
    "    for paper in data:\n",
    "        paper_idx += 1\n",
    "        paper_cnt += 1\n",
    "        str_tmp = \"\"\n",
    "        if 'title' in paper.keys():\n",
    "            str_tmp += paper['title'] + \".\"\n",
    "        if 'abstractText' in paper.keys():\n",
    "            str_tmp += \" \" + paper['abstractText']\n",
    "        # append title+abstract to sentences_list\n",
    "        if str_tmp != \"\":\n",
    "            # split sentence by \". \"\n",
    "            sentence_ls = str_tmp.split('. ')\n",
    "            for sentence in sentence_ls:\n",
    "                # 正则过滤字符串，只保留空格、数字、字母、百分数、-、<、>、/、小数点等\n",
    "                sentence = re.sub(r'[^ \\dA-Za-z%<>/\\.-]', '', sentence)\n",
    "                # 如果句子太长，切分处理\n",
    "                if len(sentence) > 500:\n",
    "                    idx = 0\n",
    "                    while idx < len(sentence):\n",
    "                        tmp_str = sentence[idx: min(idx + 500, len(sentence))] + \" end\"\n",
    "                        if len(tmp_str) <= 8:\n",
    "                            tmp_str += ' end'\n",
    "                        if tmp_str[-1] != \".\":\n",
    "                            tmp_str += '.'\n",
    "                        sentences_list.append(tmp_str)\n",
    "                        sentence_cnt += 1\n",
    "                        idx += 500\n",
    "                else:\n",
    "                    if len(sentence) <= 8:\n",
    "                        sentence += ' end'\n",
    "                    if sentence[-1] != \".\":\n",
    "                        sentence += '.'\n",
    "                    sentences_list.append(sentence)\n",
    "                    sentence_cnt += 1\n",
    "        if 'sections' in paper.keys():\n",
    "            for section in paper['sections']:\n",
    "                if \"heading\" in section.keys():\n",
    "                    str_tmp = section['heading'] + \". \" + section['text']\n",
    "                else:\n",
    "                    str_tmp = section['text']\n",
    "                # append heading+text to sentences_list\n",
    "                if str_tmp != \"\":\n",
    "                    # split sentence by \". \"\n",
    "                    sentence_ls = str_tmp.split('. ')\n",
    "                    for sentence in sentence_ls:\n",
    "                        # 正则过滤字符串，只保留空格、数字、字母、百分数、-、<、>、/、小数点等\n",
    "                        sentence = re.sub(r'[^ \\dA-Za-z%<>/\\.-]', '', sentence)\n",
    "                        # 如果句子太长，切分处理\n",
    "                        if len(sentence) > 500:\n",
    "                            idx = 0\n",
    "                            while idx < len(sentence):\n",
    "                                tmp_str = sentence[idx: min(idx + 500, len(sentence))] + \" end\"\n",
    "                                if len(tmp_str) <= 8:\n",
    "                                    tmp_str += ' end'\n",
    "                                if tmp_str[-1] != \".\":\n",
    "                                    tmp_str += '.'\n",
    "                                sentences_list.append(tmp_str)\n",
    "                                sentence_cnt += 1\n",
    "                                idx += 500\n",
    "                        else:\n",
    "                            if len(sentence) <= 8:\n",
    "                                sentence += ' end'\n",
    "                            if sentence[-1] != \".\":\n",
    "                                sentence += '.'\n",
    "                            sentences_list.append(sentence)\n",
    "                            sentence_cnt += 1\n",
    "\n",
    "        if paper_cnt % 1000 == 0:\n",
    "            print(\"current paper count: {},  current sentence count: {}\".format(paper_cnt, sentence_cnt))\n",
    "\n",
    "    print(\"# total paper count: {},  total sentence count: {}\".format(paper_cnt, sentence_cnt))\n",
    "    # pubmed_0508: total paper count: 310,  total sentence count: 35812\n",
    "    return sentences_list, sentence_cnt\n",
    "    # paper interval: [1, 500001),  total paper count: 500000,  total sentence count: 111915016\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     extracted_data, total_data_count = extract_sentences()\n",
    "#     write_file = \"./sentences/sentence_pubmed_0508.txt\"\n",
    "#     with open(write_file, 'w') as f:\n",
    "#         for item in extracted_data:\n",
    "#             f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MGKG - SingleThread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yupeng/anaconda3/envs/llms/lib/python3.9/site-packages/revChatGPT/__init__.py:31: UserWarning: The current Python is not a recommended version, 3.10+ is recommended\n",
      "  __import__(\"warnings\").warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# entity labels:\n",
      " ['medication', 'non-medication treatment', 'clinical feature', 'disease', 'symptom', 'sign', 'subgroup', 'scale', 'ancillary test', 'comorbidity', 'clinical effect', 'adverse effect', 'sex', 'age', 'age at onset', 'history of smoking', 'history of alcohol consumption', 'level of education', 'level of income', 'latitude of residence']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "# schema_relations:\n",
      " ['adverse effect', 'alleviate', 'be superior to', 'biologics', 'caution', 'clinical effect', 'coadministration', 'combine with', 'complication', 'contraindication', 'conventional immunosuppression', 'drug intenration', 'especially alleviate', 'first-line medication', 'incompatibility', 'indication', 'medication for precaution', 'medication for treatment', 'postoperative drug', 'precaution', 'preoperative drug', 'presented with', 'second-line medication', 'steroid sparing', 'subclass', 'supportive treatment', 'surgery', 'symptomatic treatment', 'third-line medication', 'treatment', 'treatment method', 'urgent treatment', 'use of drug']\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing paragraphs:  25%|██▌       | 1/4 [00:22<01:06, 22.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paragraph_cnt 4, success_cnt 1, fail_cnt 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing paragraphs:  50%|█████     | 2/4 [00:55<00:57, 28.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paragraph_cnt 4, success_cnt 2, fail_cnt 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing paragraphs:  75%|███████▌  | 3/4 [01:19<00:26, 26.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paragraph_cnt 4, success_cnt 3, fail_cnt 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing paragraphs: 100%|██████████| 4/4 [02:50<00:00, 42.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paragraph_cnt 4, success_cnt 3, fail_cnt 1\n",
      "# paragraph_cnt: 4\n",
      "# success_cnt: 3\n",
      "# fail_cnt: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import re\n",
    "import tiktoken\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from revChatGPT.V3 import Chatbot\n",
    "from PyPDF2 import PdfReader\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"http_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = openai.api_key\n",
    "\n",
    "\n",
    "# 调用API并使用重试机制处理rate limit error和其他异常\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    for i in range(3):  # Retry the API call up to 3 times\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"I want you to act as a entity and relation extractor to help me build a medical knowledge graph from a paragraph.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "            )\n",
    "            return response.choices[0].message[\"content\"]\n",
    "        except openai.error.RateLimitError:  # If rate limit is exceeded\n",
    "            wait_time = (2 ** i) + random.random()  # Exponential backoff with jitter\n",
    "            logging.warning(f\"Rate limit exceeded. Retrying after {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        except Exception as e:  # If any other error occurs\n",
    "            logging.error(f\"API call failed: {str(e)}\")\n",
    "            return None  # Return None for failure\n",
    "    logging.error(\"Failed to call OpenAI API after multiple retries due to rate limiting.\")\n",
    "    return None  # Return None for failure\n",
    "\n",
    "\n",
    "# 从schema文件中获取本体（ontology - entity_label），20个\n",
    "def get_entity_labels():\n",
    "    entity_labels = []\n",
    "\n",
    "    # 读取excel工作表MGKG_Schema_2023-05-05.xlsx - ontology\n",
    "    df = pd.read_excel('../mgkg/MGKG_Schema_2023-05-05.xlsx', sheet_name='ontology')\n",
    "    # 按行迭代数据\n",
    "    for index, row in df.iterrows():\n",
    "        # 读取行中的每个单元格\n",
    "        entity_label = row['schema']\n",
    "        entity_labels.append(entity_label)\n",
    "\n",
    "    return entity_labels\n",
    "\n",
    "\n",
    "# 从schema文件中获取关系（relation），33个\n",
    "def get_relations():\n",
    "    relations = []\n",
    "\n",
    "    # 读取excel工作表MGKG_Schema_2023-05-05.xlsx - relations\n",
    "    df = pd.read_excel('../mgkg/MGKG_Schema_2023-05-05.xlsx', sheet_name='relations')\n",
    "    # 按行迭代数据\n",
    "    for index, row in df.iterrows():\n",
    "        # 读取行中的每个单元格\n",
    "        relation_name = row['schema']\n",
    "        relations.append(relation_name)\n",
    "\n",
    "    return relations\n",
    "\n",
    "\n",
    "def triple_extraction(paragraph: str, entity_labels: list, schema_relations: list):\n",
    "    # system_prompt = \"I want you to act as a entity and relation extractor to help me build an academic knowledge graph from several paragraphs.\"\n",
    "    # chatbot = Chatbot(api_key=openai.api_key, system_prompt=system_prompt)\n",
    "    \n",
    "    prompt1 = f\"\"\"\n",
    "I will give you a paragraph. Extract as many named entities as possible from it. Your answer should only contain a list and nothing else. \n",
    "---\n",
    "Here is an example:\n",
    "\n",
    "paragraph: \n",
    "myasthenia gravis is characterized by muscle weakness. prednisolone is a treatment for myasthenia gravis.\n",
    "\n",
    "your answer: \n",
    "[\n",
    "    \"myasthenia gravis\",\n",
    "    \"muscle weakness\",\n",
    "    \"prednisolone\"\n",
    "]\n",
    "---\n",
    "Here is the paragraph you should process:\n",
    "{paragraph}\n",
    "\"\"\"\n",
    "\n",
    "    # entity_list = chatbot.ask(prompt1)\n",
    "    entity_list = get_completion(prompt1)\n",
    "    # print(entity_list)\n",
    "    \n",
    "    prompt2 = f\"\"\"This is the entity list you have just generated:\n",
    "\n",
    "{entity_list}\n",
    "\n",
    "Classify every entity in into one of the categories in the following list. You should not classify any entity into a category that in not in the following list.\n",
    "\n",
    "{entity_labels}\n",
    "\n",
    "Your result should be a JSON dictionary with entities being the keys and categories being the values. There should be nothing in your answer except the JSON dictionary.\n",
    "---\n",
    "Here is an example:\n",
    "\n",
    "paragraph: \n",
    "myasthenia gravis is characterized by muscle weakness. prednisolone is a treatment for myasthenia gravis.\n",
    "\n",
    "entity list:\n",
    "[\n",
    "    \"myasthenia gravis\",\n",
    "    \"muscle weakness\",\n",
    "    \"prednisolone\"\n",
    "]\n",
    "your answer:\n",
    "{{\n",
    "    \"myasthenia gravis\": \"disease\",\n",
    "    \"muscle weakness\": \"symptom\",\n",
    "    \"prednisolone\": \"medication\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # entity_category_dict = chatbot.ask(prompt2)\n",
    "    entity_category_dict = get_completion(prompt2)\n",
    "    # print(entity_category_dict)\n",
    "    \n",
    "    prompt3 = f\"\"\"\n",
    "The following is the paragraph:\n",
    "\n",
    "{paragraph}\n",
    "\n",
    "The following is the \"entity list\" you have just generated:\n",
    "\n",
    "{entity_list}\n",
    "\n",
    "Extract as many relations as possible from the paragraph. Your result should be a list of triples and nothing else. \n",
    "The first and third element in each triple should be in the \"entity list\" you have generated and the second element should be in the following \"relation category list\". \n",
    "You should not extract any relation that the second element in it is not in the following \"relation category list\". \n",
    "The relation you choose should be precise and diverse. You shouldn't use \"treatment\" to describe all the relations.\n",
    "\n",
    "Here is the \"relation category list\":\n",
    "{schema_relations}\n",
    "\n",
    "---\n",
    "Here is an example:\n",
    "\n",
    "paragraph: \n",
    "myasthenia gravis is characterized by muscle weakness. prednisolone is a treatment for myasthenia gravis.\n",
    "\n",
    "entity list:\n",
    "[\n",
    "    \"myasthenia gravis\",\n",
    "    \"muscle weakness\",\n",
    "    \"prednisolone\"\n",
    "]\n",
    "\n",
    "your answer:\n",
    "[\n",
    "    [\"myasthenia gravis\", \"presented with\", \"muscle weakness\"],\n",
    "    [\"prednisolone\", \"treatment\", \"myasthenia gravis\"],\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # relation_list = chatbot.ask(prompt3)\n",
    "    relation_list = get_completion(prompt3)\n",
    "    # print(relation_list)\n",
    "    \n",
    "    try:\n",
    "        p_entity_list = json.loads(entity_list)\n",
    "        p_entity_category_dict = json.loads(entity_category_dict)\n",
    "        p_relation_list = json.loads(relation_list)\n",
    "        # print(\"# JSON load successful!\")\n",
    "        load_flag = True\n",
    "        return {\n",
    "            \"entity_list\": p_entity_list,\n",
    "            \"entity_category_dict\": p_entity_category_dict,\n",
    "            \"relation_list\": p_relation_list\n",
    "        }, load_flag\n",
    "    except:\n",
    "        # print(\"# JSON load failed!\")\n",
    "        load_flag = False\n",
    "        return {\n",
    "            \"entity_list\": entity_list,\n",
    "            \"entity_category_dict\": entity_category_dict,\n",
    "            \"relation_list\": relation_list\n",
    "        }, load_flag\n",
    "\n",
    "\n",
    "entity_labels = get_entity_labels()\n",
    "schema_relations = get_relations()\n",
    "print(\"# entity labels:\\n\", entity_labels)\n",
    "print(\"-\" * 100)\n",
    "print(\"# schema_relations:\\n\", schema_relations)\n",
    "print(\"-\" * 120)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "paragraph_dict = {\n",
    "    \"paragraph\": paragraph,\n",
    "    \"relations\": [\n",
    "        {\n",
    "            \"head\": head,\n",
    "            \"head_label\": head_label,\n",
    "            \"relation\": relation,\n",
    "            \"tail\": tail,\n",
    "            \"tail_label\": tail_label\n",
    "        },\n",
    "        {\n",
    "            \"head\": head,\n",
    "            \"head_label\": head_label,\n",
    "            \"relation\": relation,\n",
    "            \"tail\": tail,\n",
    "            \"tail_label\": tail_label\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "paragraph_file = \"./data/mgkg_data/paragraph_pubmed_0720_test.txt\"\n",
    "result_write_file = \"./results/mgkg_result/paragraph_pubmed_0720_test_result.json\"\n",
    "result_single_write_file = \"./results/mgkg_result/paragraph_pubmed_0720_test_result_single.json\"\n",
    "retry_write_file = \"./data/mgkg_data/paragraph_pubmed_0720_test_retry.txt\"\n",
    "\n",
    "# paragraph_file = \"./data/mgkg_data/paragraph_pubmed_0720.txt\"\n",
    "# result_write_file = \"./results/mgkg_result/paragraph_pubmed_0720_result.json\"\n",
    "# result_single_write_file = \"./results/mgkg_result/paragraph_pubmed_0720_result_single.json\"\n",
    "# retry_write_file = \"./data/mgkg_data/paragraph_pubmed_0720_retry.txt\"\n",
    "with open(paragraph_file, \"r\") as file:\n",
    "    paragraphs = file.readlines()\n",
    "\n",
    "retry_paragraphs = []   # JSON load失败的paragraph\n",
    "paragraph_dict_list = []    # 所有paragraph的三元组抽取结果\n",
    "success_cnt = 0\n",
    "fail_cnt = 0\n",
    "\n",
    "# 将抽取结果paragraph_dict持续性写入result_single_write_file文件\n",
    "with open(result_single_write_file, \"w\", newline='\\n') as wrt_single_file:\n",
    "    for paragraph in tqdm(paragraphs, total=len(paragraphs), desc='Processing paragraphs'):\n",
    "        # print(paragraph.strip())  # 输出每一行内容（去除换行符）\n",
    "        result, load_flag = triple_extraction(paragraph, entity_labels, schema_relations)\n",
    "        if load_flag:   # JSON load成功\n",
    "            success_cnt += 1\n",
    "            entities = result['entity_list']\n",
    "            entity_labels = result['entity_category_dict']\n",
    "            relations = result['relation_list']\n",
    "            relation_dict_list = []  # 每个paragraph抽取的的所有relation\n",
    "            for item in relations:\n",
    "                head = item[0]\n",
    "                relation = item[1]\n",
    "                tail = item[2]\n",
    "                head_lebal = \"\"\n",
    "                tail_label = \"\"\n",
    "                # print(f'{head}, {relation}, {tail}')\n",
    "                entity_keys = entity_labels.keys()\n",
    "                for key in entity_keys:\n",
    "                    if key in head:\n",
    "                        head_label = entity_labels[key]\n",
    "                    if key in tail:\n",
    "                        tail_label = entity_labels[key]\n",
    "                relation_dict = {   # paragraph中抽取的一个relation\n",
    "                    \"head\": head,\n",
    "                    \"head_label\": head_label,\n",
    "                    \"relation\": relation,\n",
    "                    \"tail\": tail,\n",
    "                    \"tail_label\": tail_label\n",
    "                }\n",
    "                relation_dict_list.append(relation_dict)\n",
    "            paragraph_dict = {\n",
    "                \"paragraph\": paragraph,\n",
    "                \"relations\": relation_dict_list\n",
    "            }\n",
    "            paragraph_dict_list.append(paragraph_dict)\n",
    "        else:   # JSON load失败\n",
    "            fail_cnt += 1\n",
    "            retry_paragraphs.append(paragraph)\n",
    "            paragraph_dict = {\n",
    "                \"paragraph\": paragraph,\n",
    "                \"relations\": \"failure\"\n",
    "            }\n",
    "            paragraph_dict_list.append(paragraph_dict)\n",
    "        json.dump(paragraph_dict, wrt_single_file, indent=4)\n",
    "        wrt_single_file.write('\\n')  # 添加换行符\n",
    "        print(\"# paragraph_cnt {}, success_cnt {}, fail_cnt {}\".format(len(paragraphs), success_cnt, fail_cnt))\n",
    "\n",
    "\n",
    "print(\"# paragraph_cnt:\", len(paragraphs))\n",
    "print(\"# success_cnt:\", success_cnt)\n",
    "print(\"# fail_cnt:\", fail_cnt)\n",
    "\n",
    "# 将抽取结果paragraph_dict_list一次性写入文件\n",
    "with open(result_write_file, \"w\") as json_file:\n",
    "    json.dump(paragraph_dict_list, json_file, indent=4)\n",
    "\n",
    "# 将JSON load失败的paragraph一次性写入新文件\n",
    "with open(retry_write_file, 'w') as retry_file:\n",
    "    for item in retry_paragraphs:\n",
    "        retry_file.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MGKG - MultiThread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "import re\n",
    "import tiktoken\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from revChatGPT.V3 import Chatbot\n",
    "from PyPDF2 import PdfReader\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"http_proxy\"] = \"http://10.10.1.3:10001\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.10.1.3:10001\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = openai.api_key\n",
    "\n",
    "\n",
    "# 调用API并使用重试机制处理rate limit error和其他异常\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    for i in range(3):  # Retry the API call up to 3 times\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"I want you to act as a entity and relation extractor to help me build a medical knowledge graph from a paragraph.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "                ]\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "            )\n",
    "            return response.choices[0].message[\"content\"]\n",
    "        except openai.error.RateLimitError:  # If rate limit is exceeded\n",
    "            wait_time = (2 ** i) + random.random()  # Exponential backoff with jitter\n",
    "            logging.warning(f\"Rate limit exceeded. Retrying after {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        except Exception as e:  # If any other error occurs\n",
    "            logging.error(f\"API call failed: {str(e)}\")\n",
    "            return None  # Return None for failure\n",
    "    logging.error(\"Failed to call OpenAI API after multiple retries due to rate limiting.\")\n",
    "    return None  # Return None for failure\n",
    "\n",
    "\n",
    "# 从schema文件中获取本体（ontology - entity_label），20个\n",
    "def get_entity_labels():\n",
    "    entity_labels = []\n",
    "\n",
    "    # 读取excel工作表MGKG_Schema_2023-05-05.xlsx - ontology\n",
    "    df = pd.read_excel('../mgkg/MGKG_Schema_2023-05-05.xlsx', sheet_name='ontology')\n",
    "    # 按行迭代数据\n",
    "    for index, row in df.iterrows():\n",
    "        # 读取行中的每个单元格\n",
    "        entity_label = row['schema']\n",
    "        entity_labels.append(entity_label)\n",
    "\n",
    "    return entity_labels\n",
    "\n",
    "\n",
    "# 从schema文件中获取关系（relation），33个\n",
    "def get_relations():\n",
    "    relations = []\n",
    "\n",
    "    # 读取excel工作表MGKG_Schema_2023-05-05.xlsx - relations\n",
    "    df = pd.read_excel('../mgkg/MGKG_Schema_2023-05-05.xlsx', sheet_name='relations')\n",
    "    # 按行迭代数据\n",
    "    for index, row in df.iterrows():\n",
    "        # 读取行中的每个单元格\n",
    "        relation_name = row['schema']\n",
    "        relations.append(relation_name)\n",
    "\n",
    "    return relations\n",
    "\n",
    "\n",
    "def triple_extraction(paragraph: str, entity_labels: list, schema_relations: list):\n",
    "    # system_prompt = \"I want you to act as a entity and relation extractor to help me build an academic knowledge graph from several paragraphs.\"\n",
    "    # chatbot = Chatbot(api_key=openai.api_key, system_prompt=system_prompt)\n",
    "    \n",
    "    prompt1 = f\"\"\"\n",
    "I will give you a paragraph. Extract as many named entities as possible from it. Your answer should only contain a list and nothing else. \n",
    "---\n",
    "Here is an example:\n",
    "\n",
    "paragraph: \n",
    "myasthenia gravis is characterized by muscle weakness. prednisolone is a treatment for myasthenia gravis.\n",
    "\n",
    "your answer: \n",
    "[\n",
    "    \"myasthenia gravis\",\n",
    "    \"muscle weakness\",\n",
    "    \"prednisolone\"\n",
    "]\n",
    "---\n",
    "Here is the paragraph you should process:\n",
    "{paragraph}\n",
    "\"\"\"\n",
    "\n",
    "    # entity_list = chatbot.ask(prompt1)\n",
    "    entity_list = get_completion(prompt1)\n",
    "    # print(entity_list)\n",
    "    \n",
    "    prompt2 = f\"\"\"This is the entity list you have just generated:\n",
    "\n",
    "{entity_list}\n",
    "\n",
    "Classify every entity in into one of the categories in the following list. You should not classify any entity into a category that in not in the following list.\n",
    "\n",
    "{entity_labels}\n",
    "\n",
    "Your result should be a JSON dictionary with entities being the keys and categories being the values. There should be nothing in your answer except the JSON dictionary.\n",
    "---\n",
    "Here is an example:\n",
    "\n",
    "paragraph: \n",
    "myasthenia gravis is characterized by muscle weakness. prednisolone is a treatment for myasthenia gravis.\n",
    "\n",
    "entity list:\n",
    "[\n",
    "    \"myasthenia gravis\",\n",
    "    \"muscle weakness\",\n",
    "    \"prednisolone\"\n",
    "]\n",
    "your answer:\n",
    "{{\n",
    "    \"myasthenia gravis\": \"disease\",\n",
    "    \"muscle weakness\": \"symptom\",\n",
    "    \"prednisolone\": \"medication\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    # entity_category_dict = chatbot.ask(prompt2)\n",
    "    entity_category_dict = get_completion(prompt2)\n",
    "    # print(entity_category_dict)\n",
    "    \n",
    "    prompt3 = f\"\"\"\n",
    "The following is the paragraph:\n",
    "\n",
    "{paragraph}\n",
    "\n",
    "The following is the \"entity list\" you have just generated:\n",
    "\n",
    "{entity_list}\n",
    "\n",
    "Extract as many relations as possible from the paragraph. Your result should be a list of triples and nothing else. \n",
    "The first and third element in each triple should be in the \"entity list\" you have generated and the second element should be in the following \"relation category list\". \n",
    "You should not extract any relation that the second element in it is not in the following \"relation category list\". \n",
    "The relation you choose should be precise and diverse. You shouldn't use \"treatment\" to describe all the relations.\n",
    "\n",
    "Here is the \"relation category list\":\n",
    "{schema_relations}\n",
    "\n",
    "---\n",
    "Here is an example:\n",
    "\n",
    "paragraph: \n",
    "myasthenia gravis is characterized by muscle weakness. prednisolone is a treatment for myasthenia gravis.\n",
    "\n",
    "entity list:\n",
    "[\n",
    "    \"myasthenia gravis\",\n",
    "    \"muscle weakness\",\n",
    "    \"prednisolone\"\n",
    "]\n",
    "\n",
    "your answer:\n",
    "[\n",
    "    [\"myasthenia gravis\", \"presented with\", \"muscle weakness\"],\n",
    "    [\"prednisolone\", \"treatment\", \"myasthenia gravis\"],\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    # relation_list = chatbot.ask(prompt3)\n",
    "    relation_list = get_completion(prompt3)\n",
    "    # print(relation_list)\n",
    "    \n",
    "    try:\n",
    "        p_entity_list = json.loads(entity_list)\n",
    "        p_entity_category_dict = json.loads(entity_category_dict)\n",
    "        p_relation_list = json.loads(relation_list)\n",
    "        # print(\"# JSON load successful!\")\n",
    "        load_flag = True\n",
    "        return {\n",
    "            \"entity_list\": p_entity_list,\n",
    "            \"entity_category_dict\": p_entity_category_dict,\n",
    "            \"relation_list\": p_relation_list\n",
    "        }, load_flag, paragraph\n",
    "    except:\n",
    "        # print(\"# JSON load failed!\")\n",
    "        load_flag = False\n",
    "        return {\n",
    "            \"entity_list\": entity_list,\n",
    "            \"entity_category_dict\": entity_category_dict,\n",
    "            \"relation_list\": relation_list\n",
    "        }, load_flag, paragraph\n",
    "\n",
    "\n",
    "entity_labels = get_entity_labels()\n",
    "schema_relations = get_relations()\n",
    "print(\"# entity labels:\\n\", entity_labels)\n",
    "print(\"-\" * 100)\n",
    "print(\"# schema_relations:\\n\", schema_relations)\n",
    "print(\"-\" * 120)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "paragraph_dict = {\n",
    "    \"paragraph\": paragraph,\n",
    "    \"relations\": [\n",
    "        {\n",
    "            \"head\": head,\n",
    "            \"head_label\": head_label,\n",
    "            \"relation\": relation,\n",
    "            \"tail\": tail,\n",
    "            \"tail_label\": tail_label\n",
    "        },\n",
    "        {\n",
    "            \"head\": head,\n",
    "            \"head_label\": head_label,\n",
    "            \"relation\": relation,\n",
    "            \"tail\": tail,\n",
    "            \"tail_label\": tail_label\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "# paragraph_file = \"./data/mgkg_data/paragraph_pubmed_0720_test.txt\"\n",
    "# result_write_file = \"./results/mgkg_result/paragraph_pubmed_0720_test_result.json\"\n",
    "# retry_write_file = \"./data/mgkg_data/paragraph_pubmed_0720_test_retry.txt\"\n",
    "paragraph_file = \"./data/mgkg_data/paragraph_pubmed_0720.txt\"\n",
    "result_write_file = \"./results/mgkg_result/paragraph_pubmed_0720_result.json\"\n",
    "result_single_write_file = \"./results/mgkg_result/paragraph_pubmed_0720_result_single.json\"\n",
    "retry_write_file = \"./data/mgkg_data/paragraph_pubmed_0720_retry.txt\"\n",
    "with open(paragraph_file, \"r\") as file:\n",
    "    paragraphs = file.readlines()\n",
    "\n",
    "retry_paragraphs = []   # JSON load失败的paragraph\n",
    "paragraph_dict_list = []    # 所有paragraph的三元组抽取结果\n",
    "success_cnt = 0\n",
    "fail_cnt = 0\n",
    "\n",
    "# 将抽取结果paragraph_dict持续性写入result_single_write_file文件\n",
    "with open(result_single_write_file, \"w\", newline='\\n') as wrt_single_file:\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        futures = {executor.submit(triple_extraction, paragraph, entity_labels, schema_relations): paragraph for paragraph in paragraphs}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc='Processing paragraphs'):\n",
    "            # 收集完成的线程处理好的结果\n",
    "            result, load_flag, paragraph = future.result()\n",
    "            if load_flag:   # JSON load成功\n",
    "                success_cnt += 1\n",
    "                entities = result['entity_list']\n",
    "                entity_labels = result['entity_category_dict']\n",
    "                relations = result['relation_list']\n",
    "                relation_dict_list = []  # 每个paragraph抽取的的所有relation\n",
    "                for item in relations:\n",
    "                    head = item[0]\n",
    "                    relation = item[1]\n",
    "                    tail = item[2]\n",
    "                    head_lebal = \"\"\n",
    "                    tail_label = \"\"\n",
    "                    # print(f'{head}, {relation}, {tail}')\n",
    "                    entity_keys = entity_labels.keys()\n",
    "                    for key in entity_keys:\n",
    "                        if key in head:\n",
    "                            head_label = entity_labels[key]\n",
    "                        if key in tail:\n",
    "                            tail_label = entity_labels[key]\n",
    "                    relation_dict = {   # paragraph中抽取的一个relation\n",
    "                        \"head\": head,\n",
    "                        \"head_label\": head_label,\n",
    "                        \"relation\": relation,\n",
    "                        \"tail\": tail,\n",
    "                        \"tail_label\": tail_label\n",
    "                    }\n",
    "                    relation_dict_list.append(relation_dict)\n",
    "                paragraph_dict = {\n",
    "                    \"paragraph\": paragraph,\n",
    "                    \"relations\": relation_dict_list\n",
    "                }\n",
    "                paragraph_dict_list.append(paragraph_dict)\n",
    "            else:   # JSON load失败\n",
    "                fail_cnt += 1\n",
    "                retry_paragraphs.append(paragraph)\n",
    "                paragraph_dict = {\n",
    "                    \"paragraph\": paragraph,\n",
    "                    \"relations\": \"failure\"\n",
    "                }\n",
    "                paragraph_dict_list.append(paragraph_dict)\n",
    "            json.dump(paragraph_dict, wrt_single_file, indent=4)\n",
    "            wrt_single_file.write('\\n')  # 添加换行符\n",
    "            print(\"# paragraph_cnt {}, success_cnt {}, fail_cnt {}\".format(len(paragraphs), success_cnt, fail_cnt))\n",
    "\n",
    "\n",
    "print(\"# paragraph_cnt:\", len(paragraphs))\n",
    "print(\"# success_cnt:\", success_cnt)\n",
    "print(\"# fail_cnt:\", fail_cnt)\n",
    "\n",
    "# 将抽取结果paragraph_dict_list一次性写入文件\n",
    "with open(result_write_file, \"w\") as json_file:\n",
    "    json.dump(paragraph_dict_list, json_file, indent=4)\n",
    "\n",
    "# 将JSON load失败的paragraph一次性写入新文件\n",
    "with open(retry_write_file, 'w') as retry_file:\n",
    "    for item in retry_paragraphs:\n",
    "        retry_file.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MGKG - Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: <class 'list'> 1293\n",
      "result (reverse): <class 'list'> 1318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1839/1839 [00:00<00:00, 6139.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation_cnt: 34944 | paragraph_cnt: 1495\n",
      "result (merged): <class 'list'> 1495\n",
      "result (retry): <class 'list'> 344\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "file_path = './results/mgkg_result/paragraph_pubmed_0720_result_single.json'\n",
    "file_path_2 = './results/mgkg_result/paragraph_pubmed_0720_result_single_reverse.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"result:\", type(data), len(data))\n",
    "\n",
    "with open(file_path_2, 'r') as f_2:\n",
    "    data_2 = json.load(f_2)\n",
    "print(\"result (reverse):\", type(data_2), len(data_2))\n",
    "\n",
    "\n",
    "paragraph_file = \"./data/mgkg_data/paragraph_pubmed_0720.txt\"\n",
    "result_write_file = \"./results/mgkg_result/paragraph_pubmed_0720_result.json\"\n",
    "retry_write_file = \"./data/mgkg_data/paragraph_pubmed_0720_retry.txt\"\n",
    "\n",
    "with open(paragraph_file, \"r\") as file:\n",
    "    paragraphs = file.readlines()\n",
    "paragraph_dict_list = []    # 所有paragraph的三元组抽取结果\n",
    "retry_paragraphs = []   # JSON load失败的paragraph\n",
    "relation_cnt = 0    # 抽取出的三元组数量\n",
    "for paragraph in tqdm(paragraphs):\n",
    "    find_flag = False\n",
    "    # 先在正向的结果中找\n",
    "    for item in data:\n",
    "        if paragraph == item['paragraph'] and item['relations'] != \"failure\":\n",
    "            find_flag = True\n",
    "            paragraph_dict = item\n",
    "            relation_cnt += len(paragraph_dict['relations'])\n",
    "            paragraph_dict_list.append(paragraph_dict)\n",
    "            break\n",
    "    # 如果正向的结果中没有找到，再在反向的结果中找\n",
    "    if not find_flag:\n",
    "        for item in data_2:\n",
    "            if paragraph == item['paragraph'] and item['relations'] != \"failure\":\n",
    "                find_flag = True\n",
    "                paragraph_dict = item\n",
    "                relation_cnt += len(paragraph_dict['relations'])\n",
    "                paragraph_dict_list.append(paragraph_dict)\n",
    "                break\n",
    "    # 如果正向和反向的结果中都没有找到，将paragraph加入retry_paragraphs\n",
    "    if not find_flag:\n",
    "        retry_paragraphs.append(paragraph)\n",
    "\n",
    "print(\"relation_cnt:\", relation_cnt, \"| paragraph_cnt:\", len(paragraph_dict_list))\n",
    "print(\"result (merged):\", type(paragraph_dict_list),len(paragraph_dict_list))\n",
    "print(\"result (retry):\", type(retry_paragraphs), len(retry_paragraphs))\n",
    "\n",
    "# 将抽取结果paragraph_dict_list一次性写入文件\n",
    "with open(result_write_file, \"w\") as json_file:\n",
    "    json.dump(paragraph_dict_list, json_file, indent=4)\n",
    "\n",
    "# 将JSON load失败的paragraph一次性写入新文件\n",
    "with open(retry_write_file, 'w') as retry_file:\n",
    "    for item in retry_paragraphs:\n",
    "        retry_file.write(\"%s\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: <class 'list'> 1495\n",
      "result (reverse): <class 'list'> 267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1839/1839 [00:00<00:00, 8281.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation_cnt: 43283 | paragraph_cnt: 1762\n",
      "result (merged): <class 'list'> 1762\n",
      "result (retry): <class 'list'> 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "file_path = './results/mgkg_result/paragraph_pubmed_0720_result.json'\n",
    "file_path_2 = './results/mgkg_result/paragraph_pubmed_0720_retry_result.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"result:\", type(data), len(data))\n",
    "\n",
    "with open(file_path_2, 'r') as f_2:\n",
    "    data_2 = json.load(f_2)\n",
    "print(\"result (reverse):\", type(data_2), len(data_2))\n",
    "\n",
    "\n",
    "paragraph_file = \"./data/mgkg_data/paragraph_pubmed_0720.txt\"\n",
    "result_write_file = \"./results/mgkg_result/paragraph_pubmed_0720_retry_result.json\"\n",
    "retry_write_file = \"./data/mgkg_data/paragraph_pubmed_0720_retry.txt\"\n",
    "\n",
    "with open(paragraph_file, \"r\") as file:\n",
    "    paragraphs = file.readlines()\n",
    "paragraph_dict_list = []    # 所有paragraph的三元组抽取结果\n",
    "retry_paragraphs = []   # JSON load失败的paragraph\n",
    "relation_cnt = 0    # 抽取出的三元组数量\n",
    "for paragraph in tqdm(paragraphs):\n",
    "    find_flag = False\n",
    "    # 先在正向的结果中找\n",
    "    for item in data:\n",
    "        if paragraph == item['paragraph'] and item['relations'] != \"failure\":\n",
    "            find_flag = True\n",
    "            paragraph_dict = item\n",
    "            relation_cnt += len(paragraph_dict['relations'])\n",
    "            paragraph_dict_list.append(paragraph_dict)\n",
    "            break\n",
    "    # 如果正向的结果中没有找到，再在反向的结果中找\n",
    "    if not find_flag:\n",
    "        for item in data_2:\n",
    "            if paragraph == item['paragraph'] and item['relations'] != \"failure\":\n",
    "                find_flag = True\n",
    "                paragraph_dict = item\n",
    "                relation_cnt += len(paragraph_dict['relations'])\n",
    "                paragraph_dict_list.append(paragraph_dict)\n",
    "                break\n",
    "    # 如果正向和反向的结果中都没有找到，将paragraph加入retry_paragraphs\n",
    "    if not find_flag:\n",
    "        retry_paragraphs.append(paragraph)\n",
    "\n",
    "print(\"relation_cnt:\", relation_cnt, \"| paragraph_cnt:\", len(paragraph_dict_list))\n",
    "print(\"result (merged):\", type(paragraph_dict_list),len(paragraph_dict_list))\n",
    "print(\"result (retry):\", type(retry_paragraphs), len(retry_paragraphs))\n",
    "\n",
    "# 将抽取结果paragraph_dict_list一次性写入文件\n",
    "# with open(result_write_file, \"w\") as json_file:\n",
    "#     json.dump(paragraph_dict_list, json_file, indent=4)\n",
    "\n",
    "# 将JSON load失败的paragraph一次性写入新文件\n",
    "with open(retry_write_file, 'w') as retry_file:\n",
    "    for item in retry_paragraphs:\n",
    "        retry_file.write(\"%s\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph: <class 'list'> || 1839\n",
      "--------------------------------------------------\n",
      "result: <class 'list'> || 1495\n",
      "relation_cnt: 34944 | paragraph_cnt: 1495\n",
      "--------------------------------------------------\n",
      "result: <class 'list'> || 267\n",
      "relation_cnt: 8339 | paragraph_cnt: 267\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "paragraph_file = \"./data/mgkg_data/paragraph_pubmed_0720.txt\"\n",
    "with open(paragraph_file, \"r\") as file:\n",
    "    paragraphs = file.readlines()\n",
    "print(\"paragraph:\", type(paragraphs), \"||\", len(paragraphs))\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "file_path = './results/mgkg_result/paragraph_pubmed_0720_result.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"result:\", type(data), \"||\", len(data))\n",
    "paragraph_cnt = len(data)\n",
    "relation_cnt = 0\n",
    "for item in data:\n",
    "    relation_cnt += len(item['relations'])\n",
    "print(\"relation_cnt:\", relation_cnt, \"| paragraph_cnt:\", paragraph_cnt)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "file_path = './results/mgkg_result/paragraph_pubmed_0720_retry_result.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"result:\", type(data), \"||\", len(data))\n",
    "paragraph_cnt = len(data)\n",
    "relation_cnt = 0\n",
    "for item in data:\n",
    "    relation_cnt += len(item['relations'])\n",
    "print(\"relation_cnt:\", relation_cnt, \"| paragraph_cnt:\", paragraph_cnt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test OpenAI api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "The OpenAI account associated with this API key has been deactivated. If you are the developer for this OpenAI app, please check your email for more information. If you are seeing this error while using another app or site, please reach out to them for more help.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ[\"http_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "# os.environ[\"https_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "\n",
    "# 候选的api-key\n",
    "candidate_keys = [\n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\", \n",
    "    \"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\"\n",
    "]\n",
    "\n",
    "# 实际可用的api-key\n",
    "available_keys = []\n",
    "\n",
    "# 测试api-key是否可用，用openai.ChatCompletion.create()方法测试\n",
    "def test_api_key(api_key):\n",
    "    openai.api_key = api_key\n",
    "    try:\n",
    "        msgs = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant..\"},\n",
    "                {\"role\": \"user\", \"content\": \"hello\"}\n",
    "                ]\n",
    "        response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=msgs, temperature=0)\n",
    "        print(response.choices[0].message[\"content\"])\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "for key in candidate_keys:\n",
    "    if test_api_key(key):\n",
    "        available_keys.append(key)\n",
    "print(available_keys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Task Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: task_generation\n",
    "# Author: Reacubeth\n",
    "# Time: 2023/9/10 10:49\n",
    "# Mail: noverfitting@gmail.com\n",
    "# Site: www.omegaxyz.com\n",
    "# -- coding: utf-8 --**\n",
    "\n",
    "import openai\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"http_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "os.environ[\"https_proxy\"] = \"http://10.10.1.3:10000\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# 调用API并使用重试机制处理rate limit error和其他异常\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    for i in range(3):  # Retry the API call up to 3 times\n",
    "        try:\n",
    "            messages = [\n",
    "                # {\"role\": \"system\", \"content\": \"You are an expert of the .\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "            )\n",
    "            return response.choices[0].message[\"content\"]\n",
    "        except openai.error.RateLimitError:  # If rate limit is exceeded\n",
    "            wait_time = (2 ** i) + random.random()  # Exponential backoff with jitter\n",
    "            logging.warning(f\"Rate limit exceeded. Retrying after {wait_time} seconds.\")\n",
    "            time.sleep(wait_time)  # Wait before retrying\n",
    "        except Exception as e:  # If any other error occurs\n",
    "            logging.error(f\"API call failed: {str(e)}\")\n",
    "            return None  # Return None for failure\n",
    "    logging.error(\"Failed to call OpenAI API after multiple retries due to rate limiting.\")\n",
    "    return None  # Return None for failure\n",
    "\n",
    "\n",
    "cognitive_levels = [\"remember\", \"understand\", \"apply\", \"analyze\", \"evaluate\", \"create\"]\n",
    "knowledge_dimensions = [\"factual\", \"conceptual\", \"procedural\", \"metacognitive\"]\n",
    "\n",
    "task_examples = [\"List primary and secondary colors.\",\n",
    "                 \"Recognize symptoms of exhaustion.\",\n",
    "                 \"Recall how to perform CPR.\",\n",
    "                 \"Identify strategies for retaining information.\",\n",
    "\n",
    "                 \"Summarize features of a new product.\",\n",
    "                 \"Classify adhesives by toxicity.\",\n",
    "                 \"Clarify assembly instructions.\",\n",
    "                 \"Predict one's response to culture shock.\",\n",
    "\n",
    "                 \"Respond to frequently asked questions.\",\n",
    "                 \"Provide advice to novices.\",\n",
    "                 \"Carry out pH tests of water samples.\",\n",
    "                 \"Use techniques that match one's strengths.\",\n",
    "\n",
    "                 \"Select the most complete list of activities.\",\n",
    "                 \"Differentiate high and low culture.\",\n",
    "                 \"Integrate compliance with regulations.\",\n",
    "                 \"Deconstruct one's biases.\",\n",
    "\n",
    "                 \"Check for consistency among sources.\",\n",
    "                 \"Determine relevance of results.\",\n",
    "                 \"Judge efficiency of sampling techniques.\",\n",
    "                 \"Reflect on one's progress.\",\n",
    "\n",
    "                 \"Generate a log of daily activities.\",\n",
    "                 \"Assemble a team of experts.\",\n",
    "                 \"Design efficient project workflow.\",\n",
    "                 \"Create a learning portfolio.\"]\n",
    "\n",
    "subjects = [\"computer science\", \"biology\", \"chemistry\", \"physics\", \"mathematics\", \"psychology\", \"economics\", \"history\", \"geography\", \"politics\", \"law\", \"literature\", \"art\", \"music\"]\n",
    "task_num = 5\n",
    "example_num = 1\n",
    "# cognitive_level = cognitive_levels[2]\n",
    "# knowledge_dimension = knowledge_dimensions[2]\n",
    "# subject = subjects[0]\n",
    "\n",
    "task_genaration_prompt = \"\"\"## Task Generation for {cognitive_level} + {knowledge_dimension} in Bloom's Taxonomy\n",
    "\n",
    "Bloom's taxonomy is the most widely used classification system in the field of education, and it can be viewed from both the Cognitive Process Dimension and the Knowledge Dimension.\n",
    "\n",
    "The Cognitive Process Dimension represents a continuum of increasing cognitive complexity — from lower order thinking skills to higher order thinking skills:\n",
    "1. Remember \n",
    "    - Retrieving relevant knowledge from long-term memory.\n",
    "    - Examples of the \"Remember\" cognitive processes (including but not limited to):\n",
    "        - recognizing (identifying)\n",
    "        - recalling (retrieving)\n",
    "2. Understand\n",
    "    - Determining the meaning of instructional messages, including oral, written, and graphic communication.\n",
    "    - Examples of the \"Understand\" cognitive processes (including but not limited to):\n",
    "        - interpreting (clarifying, paraphrasing, representing, translating)\n",
    "        - exemplifying (illustrating, instantiating)\n",
    "        - classifying (categorizing, subsuming)\n",
    "        - summarizing (abstracting, generalizing)\n",
    "        - inferring (concluding, extrapolating, interpolating, predicting)\n",
    "        - comparing (contrasting, mapping, matching)\n",
    "        - explaining (constructing models)\n",
    "3. Apply\n",
    "    - Carrying out or using a procedure in a given situation.\n",
    "    - Examples of the \"Apply\" cognitive processes (including but not limited to):\n",
    "        - executing (carrying out)\n",
    "        - implementing (using)\n",
    "4. Analyze\n",
    "    - Breaking material into its constituent parts and detecting how the parts relate to one another and to an overall structure or purpose.\n",
    "    - Examples of the \"Analyze\" cognitive processes (including but not limited to):\n",
    "        - differentiating (discriminating, distinguishing, focusing, selecting)\n",
    "        - organizing (finding, coherence, integrating, outlining, parsing, structuring)\n",
    "        - attributing (deconstructing)\n",
    "5. Evaluate\n",
    "    - Making judgments based on criteria and standards.\n",
    "    - Examples of the \"Evaluate\" cognitive processes (including but not limited to):\n",
    "        - checking (coordinating, detecting, monitoring, testing)\n",
    "        - critiquing (judging)\n",
    "6. Create\n",
    "    - Putting elements together to form a novel, coherent whole or make an original product.\n",
    "    - Examples of the \"Create\" cognitive processes (including but not limited to):\n",
    "        - generating (hypothesizing)\n",
    "        - planning (designing)\n",
    "        - producing (construct)\n",
    "\n",
    "The Knowledge Dimension classiffies four types of knowledge that learners may be expected to acquire or construct — ranging from concrete to abstract:\n",
    "1. Factual\n",
    "    - The basic elements that students must know to be acquainted with a discipline or solve problems in it.\n",
    "    - Examples of the \"Factual\" knowledge (including but not limited to):\n",
    "        - knowledge of terminology\n",
    "        - knowledge of specific details and elements\n",
    "2. Conceptual\n",
    "    - The interrelationships among the basic elements within a larger structure that enable them to function together.\n",
    "    - Examples of the \"Conceptual\" knowledge (including but not limited to):\n",
    "        - knowledge of classifications and categories\n",
    "        - knowledge of principles and generalizations\n",
    "        - knowledge of theories, models, and structures\n",
    "3. Procedural\n",
    "    - How to do something; methods of inquiry, and criteria for using skills, algorithms, techniques, and methods.\n",
    "    - Examples of the \"Procedural\" knowledge (including but not limited to):\n",
    "        - knowledge of subject-specific skills and algorithms\n",
    "        - knowledge of subject-specific techniques and methods\n",
    "        - knowledge of criteria for determining when to use appropriate procedures\n",
    "4. Metacognitive\n",
    "    - Knowledge of cognition in general as well as awareness and knowledge of one's own cognition.\n",
    "    - Examples of the \"Metacognitive\" knowledge (including but not limited to):\n",
    "        - strategic knowledge\n",
    "        - knowledge about cognitive tasks, including appropriate contextual and conditional knowledge\n",
    "        - self-knowledge\n",
    "\n",
    "Job Description\n",
    "1. According to Bloom's taxonomy, give {task_num} tasks of the cognitive process of \"{cognitive_level}\" combines the knowledge dimension \"{knowledge_dimension}\" in the domain of {subject}.\n",
    "2. You should first give the task name, then give the task description. \n",
    "3. The task name should be short, which MUST NOT contain these words \"{stop_words}\".\n",
    "4. You should also carefully consider the task you generated MUST correspond to the definition of the cognitive process level \"{cognitive_level}\" and knowledge dimension \"{knowledge_dimension}\", for example, in other domain, the task could be \"{other_task}\".\n",
    "5. You MUST NOT generate duplicate tasks of other cognitive process and knowledge dimensions\".\n",
    "\n",
    "Your Response Format Example (no -ing)\n",
    "1. Task Name: <TASK NAME>\n",
    "   Task Description: <TASK DESCRIPTION>\n",
    "\"\"\"\n",
    "\n",
    "# , and finally give {example_num} multiple-choice task examples with answers\n",
    "\n",
    "# print(\"cognitive_level:\", cognitive_level)\n",
    "# print(\"knowledge_dimension:\", knowledge_dimension)\n",
    "# print(\"subject:\", subject)\n",
    "# print(\"-\" * 80)\n",
    "# response = get_completion(task_genaration_prompt)\n",
    "# print(response)\n",
    "\n",
    "json_file = \"tasks.json\"\n",
    "txt_file = \"tasks.txt\"\n",
    "json_results = []\n",
    "cnt = 0\n",
    "with open(txt_file, \"w\") as file:\n",
    "    for cognitive_level in cognitive_levels:\n",
    "        for knowledge_dimension in knowledge_dimensions:\n",
    "            subject = subjects[0]\n",
    "            stop_words_ls = [c for c in cognitive_levels if c != cognitive_level]\n",
    "            cur_template = task_genaration_prompt.format(task_num=task_num,\n",
    "                                                         cognitive_level=cognitive_level,\n",
    "                                                         knowledge_dimension=knowledge_dimension,\n",
    "                                                         subject=subject,\n",
    "                                                         example_num=example_num,\n",
    "                                                         other_task=task_examples[cnt],\n",
    "                                                         stop_words=\", \".join(stop_words_ls))\n",
    "            response = get_completion(cur_template) + '\\n'\n",
    "            # extract task name and task description respectively with regular expression\n",
    "            print('-' * 80)\n",
    "            print(\"cognitive_level:\", cognitive_level)\n",
    "            print(\"knowledge_dimension:\", knowledge_dimension)\n",
    "            print(\"task_examples:\", task_examples[cnt])\n",
    "            # print(cur_template)\n",
    "            print(response)\n",
    "            task_name = re.findall(r\"Task Name: (.*)\\n\", response)\n",
    "            task_description = re.findall(r\"Task Description: (.*)\\n\", response)\n",
    "\n",
    "            task_dict = {}\n",
    "            for i in range(len(task_name)):\n",
    "                task_dict[task_name[i]] = task_description[i]\n",
    "\n",
    "            json_dict = {\n",
    "                \"cognitive_level\": cognitive_level,\n",
    "                \"knowledge_dimension\": knowledge_dimension,\n",
    "                \"subject\": subject,\n",
    "                \"response\": response,\n",
    "                \"tasks_dict\": task_dict,\n",
    "            }\n",
    "            json_results.append(json_dict)\n",
    "            file.write(\"cognitive_level: %s\\n\" % cognitive_level)\n",
    "            file.write(\"knowledge_dimension: %s\\n\" % knowledge_dimension)\n",
    "            file.write(\"subject: %s\\n\" % subject)\n",
    "            file.write(\"task_name: %s\\n\" % task_dict)\n",
    "            file.write(\"%s\\n\" % response)\n",
    "            file.write(\"-\" * 100)\n",
    "            file.write(\"\\n\")\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "with open(json_file, \"w\") as file:\n",
    "    json.dump(json_results, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03cfac8420277217bc15eae6c08a9cabc6fb9173ce0ac6e9a0d7989c1c958163"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
